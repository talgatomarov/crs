{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "I recently applied to Canada’s Express Entry program under the Canadian Experience Class and ended up with a Comprehensive Ranking System (CRS) score of 508. I wanted to know roughly how many candidates are ahead of me in the pool, but IRCC only publishes counts by score bins. That got me curious about reconstructing the full score distribution from those binned counts. Estimating that distribution can help me infer the number of candidates scoring above any threshold—like my 508—and visualize the overall shape of scores in the pool.\n",
        "\n",
        "## Why Binning Creates Issues\n",
        "\n",
        "IRCC’s table reports candidate counts in ranges (e.g., 451–500, 501–600). While that’s useful for a broad view, it obscures the detailed distribution within each bin. Standard density estimators assume a continuous sample of individual data points, but here we only have counts per interval. If I naively applied a kernel density estimate to the bin midpoints, I’d ignore within-bin variation and get a misleading smooth curve. Instead, I treat the observed counts as coming from a discretized mixture model and recover an underlying continuous distribution.\n",
        "\n",
        "## Modeling Approach\n",
        "\n",
        "I choose a Gaussian mixture model with three components to balance flexibility and interpretability. The model:\n",
        "\n",
        "- Defines component means (mu), spreads (sigma), and weights (w).\n",
        "- Constructs a continuous mixture distribution.\n",
        "- Computes the probability mass in each score bin via the Normal CDF at the cutpoints.\n",
        "- Fits the observed bin counts with a multinomial likelihood.\n",
        "- Derives the posterior distribution over the fraction (and count) of candidates above 507.5, which approximates my rank.\n",
        "- Below is the core model code, kept close to my original implementation.\n"
      ],
      "id": "63d83c62"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pymc as pm\n",
        "import pytensor.tensor as pt\n",
        "import arviz as az\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")"
      ],
      "id": "f7030454",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define bins and observed counts\n",
        "cutpoints = np.array([300, 350, 400, 410, 420, 430, 440, 450,\n",
        "                      460, 470, 480, 490, 500, 600, 1200])\n",
        "counts = np.array([8452, 22069, 53684, 12686, 13589, 13718,\n",
        "                   15241, 14439, 15276, 16691, 16641, 12704,\n",
        "                   12429, 22435, 28])\n",
        "\n",
        "k = 3  # number of mixture components\n",
        "\n",
        "with pm.Model() as model:\n",
        "    # Priors for mixture components\n",
        "    mu = pm.Normal(\"mu\", mu=450, sigma=50, shape=k)\n",
        "    sigma = pm.HalfNormal(\"sigma\", sigma=100, shape=k)\n",
        "    w = pm.Dirichlet(\"w\", a=np.ones(k))\n",
        "\n",
        "    # Continuous mixture distribution\n",
        "    mixture = pm.NormalMixture(\"mixture\", mu=mu, sigma=sigma, w=w)\n",
        "\n",
        "    # Probability in each bin via CDF differences\n",
        "    logcdf = pm.logcdf(mixture, cutpoints)\n",
        "    cdf = pm.math.concatenate([[0], pm.math.exp(logcdf)])\n",
        "    pdf = pt.extra_ops.diff(cdf)\n",
        "\n",
        "    # Multinomial likelihood on observed counts\n",
        "    pm.Multinomial(\"counts\", p=pdf, n=counts.sum(), observed=counts)\n",
        "\n",
        "    # Derived quantity: fraction and count above my score\n",
        "    percentile = pm.Deterministic(\"percentile_above_5075\",\n",
        "                                  1 - pm.math.exp(pm.logcdf(mixture, 507.5)))\n",
        "    rank = pm.Deterministic(\"rank_above_5075\", percentile * counts.sum())"
      ],
      "id": "27dae81b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference and Diagnostics\n",
        "\n",
        "I sample 5,000 draws per chain (4 chains) using ADVI initialization and a 0.95 target acceptance rate. Then I inspect convergence and mixing.\n"
      ],
      "id": "0534dfc7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with model:\n",
        "    idata = pm.sample(draws=5000, chains=4, init=\"advi\", target_accept=0.95)\n",
        "\n",
        "# Trace plot for key parameters\n",
        "az.plot_trace(idata, var_names=[\"mu\", \"sigma\", \"w\"])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "b129350b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimated Score Distribution\n",
        "Using the posterior samples, I reconstruct the continuous density and overlay it on a histogram of synthetic draws.\n"
      ],
      "id": "72dc7b6d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "az.plot_dist(\n",
        "    idata[\"posterior\"][\"mixture\"],\n",
        "    figsize=(12, 4),\n",
        "    kind=\"hist\",\n",
        "    hist_kwargs={\"bins\": np.arange(0, 750, 10), \"alpha\": 0.7},\n",
        ")\n",
        "plt.title(\"Posterior Distribution of CRS Scores\")\n",
        "plt.xlabel(\"CRS Score\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.xticks(rotation=90)\n",
        "plt.locator_params(nbins=20)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ],
      "id": "7d53f52b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with model:\n",
        "  ppc = pm.sample_posterior_predictive(idata)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "sns.barplot(counts, ax=ax)\n",
        "ppc.posterior_predictive.plot.scatter(x=\"counts_dim_0\", y=\"counts\", color=\"k\", alpha=0.2)\n",
        "ax.set_xticklabels([f\"bin {n}\" for n in range(len(counts))])\n",
        "ax.set_title(\"Six bin discretization of N(-2, 2)\")\n",
        "plt.show()"
      ],
      "id": "c4d1e6fe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Credible Interval for My Rank\n",
        "\n",
        "Finally, I summarize the posterior for the number of candidates ahead of me (score > 507.5):\n"
      ],
      "id": "fe964d29"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extract posterior samples of rank\n",
        "rank_samples = idata.posterior[\"rank_above_5075\"].values.flatten()\n",
        "\n",
        "# Compute summary\n",
        "lower, median, upper = np.percentile(rank_samples, [2.5, 50, 97.5])\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "sns.histplot(rank_samples, bins=50, stat=\"density\", alpha=0.7, ax=ax)\n",
        "\n",
        "# Draw quantile lines\n",
        "ax.axvline(lower, linestyle='--', linewidth=2, label='2.5% Quantile')\n",
        "ax.axvline(median, linestyle='-', linewidth=2, label='Median')\n",
        "ax.axvline(upper, linestyle='--', linewidth=2, label='97.5% Quantile')\n",
        "\n",
        "# Add labels with white background boxes\n",
        "ymax = ax.get_ylim()[1]\n",
        "text_kwargs = dict(ha='center', va='bottom',\n",
        "                   fontsize=14, fontweight='bold',\n",
        "                   bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.3'))\n",
        "\n",
        "ax.text(lower, ymax * 0.05, f\"{lower:.0f}\", **text_kwargs)\n",
        "ax.text(median, ymax * 0.8, f\"{median:.0f}\", **text_kwargs)\n",
        "ax.text(upper, ymax * 0.05, f\"{upper:.0f}\", **text_kwargs)\n",
        "\n",
        "# Titles and labels\n",
        "ax.set_title(\"Posterior Distribution of Candidates Ahead\", fontsize=16, pad=15)\n",
        "ax.set_xlabel(\"Number of Candidates Scoring Above 507.5\", fontsize=14)\n",
        "ax.set_ylabel(\"Density\", fontsize=14)\n",
        "\n",
        "ax.legend(fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "18878a4e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This approach shows how to reconstruct a detailed score distribution from binned counts using Bayesian inference. The same pattern can apply to any binned data problem (credit scores, exam results, income brackets) where you need a smooth underlying estimate"
      ],
      "id": "ca525821"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}